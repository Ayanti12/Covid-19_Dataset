{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29d6d30f-e810-4546-a2ca-e79560f46657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Section 4: Handling Large Datasets (Python Code and Output)\n",
      "This section demonstrates techniques for handling large datasets, including chunk-based loading, memory optimization, and vectorized operations.\n",
      "\n",
      "## 1. Chunk-based Loading and Processing\n",
      "This method is useful when your dataset is too large to fit into memory. We process it in smaller chunks.\n",
      "\n",
      "*Note: The file 'covid_project_dataset.csv' might not be 'large' enough to strictly require chunking, but this code demonstrates the principle.*\n",
      "\n",
      "Reading 'covid_project_dataset.csv' in chunks of 50000 rows...\n",
      "\n",
      "--- Processing Chunk 1 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 305996741461\n",
      "First 5 rows of Chunk 1:\n",
      "        Date      country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "0 2020-01-05  Afghanistan        0.0        0.0     0.0         0.0   \n",
      "1 2020-01-06  Afghanistan        0.0        0.0     0.0         0.0   \n",
      "2 2020-01-07  Afghanistan        0.0        0.0     0.0         0.0   \n",
      "3 2020-01-08  Afghanistan        0.0        0.0     0.0         0.0   \n",
      "4 2020-01-09  Afghanistan        0.0        0.0     0.0         0.0   \n",
      "\n",
      "   Recovered  new_recoveries  active_cases  Population  \n",
      "0          0               0           0.0    41128772  \n",
      "1          0               0           0.0    41128772  \n",
      "2          0               0           0.0    41128772  \n",
      "3          0               0           0.0    41128772  \n",
      "4          0               0           0.0    41128772  \n",
      "\n",
      "--- Processing Chunk 2 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 132119289277\n",
      "First 5 rows of Chunk 2:\n",
      "            Date   country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "50000 2023-12-29  Botswana   330417.0        0.0  2800.0         0.0   \n",
      "50001 2023-12-30  Botswana   330417.0        0.0  2800.0         0.0   \n",
      "50002 2023-12-31  Botswana   330417.0        0.0  2800.0         0.0   \n",
      "50003 2024-01-01  Botswana   330417.0        0.0  2800.0         0.0   \n",
      "50004 2024-01-02  Botswana   330417.0        0.0  2800.0         0.0   \n",
      "\n",
      "       Recovered  new_recoveries  active_cases  Population  \n",
      "50000          0               0      327617.0     2630300  \n",
      "50001          0               0      327617.0     2630300  \n",
      "50002          0               0      327617.0     2630300  \n",
      "50003          0               0      327617.0     2630300  \n",
      "50004          0               0      327617.0     2630300  \n",
      "\n",
      "--- Processing Chunk 3 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 489414798714\n",
      "First 5 rows of Chunk 3:\n",
      "             Date   country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "100000 2023-05-23  Dominica    15769.0        0.0    74.0         0.0   \n",
      "100001 2023-05-24  Dominica    15769.0        0.0    74.0         0.0   \n",
      "100002 2023-05-25  Dominica    15769.0        0.0    74.0         0.0   \n",
      "100003 2023-05-26  Dominica    15769.0        0.0    74.0         0.0   \n",
      "100004 2023-05-27  Dominica    15769.0        0.0    74.0         0.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "100000          0               0       15695.0       72758  \n",
      "100001          0               0       15695.0       72758  \n",
      "100002          0               0       15695.0       72758  \n",
      "100003          0               0       15695.0       72758  \n",
      "100004          0               0       15695.0       72758  \n",
      "\n",
      "--- Processing Chunk 4 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 528697926987\n",
      "First 5 rows of Chunk 4:\n",
      "             Date country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "150000 2022-10-15    Guam    49009.0        0.0   401.0         0.0   \n",
      "150001 2022-10-16    Guam    49009.0        0.0   401.0         0.0   \n",
      "150002 2022-10-17    Guam    49009.0        0.0   401.0         0.0   \n",
      "150003 2022-10-18    Guam    49009.0        0.0   401.0         0.0   \n",
      "150004 2022-10-19    Guam    49009.0        0.0   401.0         0.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "150000          0               0       48608.0      171783  \n",
      "150001          0               0       48608.0      171783  \n",
      "150002          0               0       48608.0      171783  \n",
      "150003          0               0       48608.0      171783  \n",
      "150004          0               0       48608.0      171783  \n",
      "\n",
      "--- Processing Chunk 5 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 125241650532\n",
      "First 5 rows of Chunk 5:\n",
      "             Date country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "200000 2022-03-09  Latvia   701538.0        0.0  5801.0         0.0   \n",
      "200001 2022-03-10  Latvia   701538.0        0.0  5801.0         0.0   \n",
      "200002 2022-03-11  Latvia   701538.0        0.0  5801.0         0.0   \n",
      "200003 2022-03-12  Latvia   701538.0        0.0  5801.0         0.0   \n",
      "200004 2022-03-13  Latvia   742334.0    40796.0  5911.0       110.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "200000          0               0      695737.0     1850654  \n",
      "200001          0               0      695737.0     1850654  \n",
      "200002          0               0      695737.0     1850654  \n",
      "200003          0               0      695737.0     1850654  \n",
      "200004          0               0      736423.0     1850654  \n",
      "\n",
      "--- Processing Chunk 6 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 180874058395\n",
      "First 5 rows of Chunk 6:\n",
      "             Date  country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "250000 2021-08-01  Myanmar   302665.0    33140.0  9731.0      2620.0   \n",
      "250001 2021-08-02  Myanmar   302665.0        0.0  9731.0         0.0   \n",
      "250002 2021-08-03  Myanmar   302665.0        0.0  9731.0         0.0   \n",
      "250003 2021-08-04  Myanmar   302665.0        0.0  9731.0         0.0   \n",
      "250004 2021-08-05  Myanmar   302665.0        0.0  9731.0         0.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "250000          0               0      292934.0    54179312  \n",
      "250001          0               0      292934.0    54179312  \n",
      "250002          0               0      292934.0    54179312  \n",
      "250003          0               0      292934.0    54179312  \n",
      "250004          0               0      292934.0    54179312  \n",
      "\n",
      "--- Processing Chunk 7 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 140338071553\n",
      "First 5 rows of Chunk 7:\n",
      "             Date country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "300000 2020-12-24   Qatar   141858.0        0.0   243.0         0.0   \n",
      "300001 2020-12-25   Qatar   141858.0        0.0   243.0         0.0   \n",
      "300002 2020-12-26   Qatar   141858.0        0.0   243.0         0.0   \n",
      "300003 2020-12-27   Qatar   142903.0     1045.0   244.0         1.0   \n",
      "300004 2020-12-28   Qatar   142903.0        0.0   244.0         0.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "300000          0               0      141615.0     2695131  \n",
      "300001          0               0      141615.0     2695131  \n",
      "300002          0               0      141615.0     2695131  \n",
      "300003          0               0      142659.0     2695131  \n",
      "300004          0               0      142659.0     2695131  \n",
      "\n",
      "--- Processing Chunk 8 ---\n",
      "Chunk shape: (50000, 10)\n",
      "Confirmed cases in this chunk: 403139396029\n",
      "First 5 rows of Chunk 8:\n",
      "             Date country  Confirmed  new_cases   Deaths  new_deaths  \\\n",
      "350000 2020-05-18   Spain   238797.0        0.0  28378.0         0.0   \n",
      "350001 2020-05-19   Spain   238797.0        0.0  28378.0         0.0   \n",
      "350002 2020-05-20   Spain   238797.0        0.0  28378.0         0.0   \n",
      "350003 2020-05-21   Spain   238797.0        0.0  28378.0         0.0   \n",
      "350004 2020-05-22   Spain   238797.0        0.0  28378.0         0.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "350000          0               0      210419.0    47558632  \n",
      "350001          0               0      210419.0    47558632  \n",
      "350002          0               0      210419.0    47558632  \n",
      "350003          0               0      210419.0    47558632  \n",
      "350004          0               0      210419.0    47558632  \n",
      "\n",
      "--- Processing Chunk 9 ---\n",
      "Chunk shape: (11804, 10)\n",
      "Confirmed cases in this chunk: 727234919798\n",
      "First 5 rows of Chunk 9:\n",
      "             Date  country  Confirmed  new_cases  Deaths  new_deaths  \\\n",
      "400000 2024-05-11  Vatican       26.0        0.0     0.0         0.0   \n",
      "400001 2024-05-12  Vatican       26.0        0.0     0.0         0.0   \n",
      "400002 2024-05-13  Vatican       26.0        0.0     0.0         0.0   \n",
      "400003 2024-05-14  Vatican       26.0        0.0     0.0         0.0   \n",
      "400004 2024-05-15  Vatican       26.0        0.0     0.0         0.0   \n",
      "\n",
      "        Recovered  new_recoveries  active_cases  Population  \n",
      "400000          0               0          26.0         808  \n",
      "400001          0               0          26.0         808  \n",
      "400002          0               0          26.0         808  \n",
      "400003          0               0          26.0         808  \n",
      "400004          0               0          26.0         808  \n",
      "\n",
      "Finished processing all chunks. Total rows processed: 411804\n",
      "Total confirmed cases aggregated from chunks: 3033056852746\n",
      "\n",
      "## 2. Optimize Memory Usage\n",
      "We'll load the full dataset and then apply data type optimizations to reduce memory footprint.\n",
      "\n",
      "Initial memory usage of DataFrame: 51.26 MB\n",
      "  Converted 'country' (object) to 'category' (Unique: 246).\n",
      "  Downcasted 'Deaths' (float) from float64 to float32.\n",
      "  Downcasted 'new_deaths' (float) from float64 to float32.\n",
      "  Downcasted 'Recovered' (int) from int64 to int8.\n",
      "  Downcasted 'new_recoveries' (int) from int64 to int8.\n",
      "\n",
      "Final memory usage of DataFrame after optimization: 20.44 MB\n",
      "Memory reduction: 60.12%\n",
      "\n",
      "DataFrame dtypes after optimization:\n",
      "Date              datetime64[ns]\n",
      "country                 category\n",
      "Confirmed                float64\n",
      "new_cases                float64\n",
      "Deaths                   float32\n",
      "new_deaths               float32\n",
      "Recovered                   int8\n",
      "new_recoveries              int8\n",
      "active_cases             float64\n",
      "Population                 int64\n",
      "dtype: object\n",
      "\n",
      "## 3. Implement Vectorized Operations using NumPy for Better Performance\n",
      "Pandas and NumPy are designed for vectorized operations, which are significantly faster than explicit Python loops for large datasets.\n",
      "Many operations in previous sections (like `.diff()`, `.groupby().agg()`, arithmetic operations on columns) already leverage vectorization.\n",
      "\n",
      "Demonstrating vectorized operation: Calculating 'Cases per Million' (approximate, using total population).\n",
      "\n",
      "First 5 rows with 'Cases_Per_Million' calculated:\n",
      "       country       Date  Confirmed  Population  Cases_Per_Million\n",
      "0  Afghanistan 2020-01-05        0.0    41128772                0.0\n",
      "1  Afghanistan 2020-01-06        0.0    41128772                0.0\n",
      "2  Afghanistan 2020-01-07        0.0    41128772                0.0\n",
      "3  Afghanistan 2020-01-08        0.0    41128772                0.0\n",
      "4  Afghanistan 2020-01-09        0.0    41128772                0.0\n",
      "\n",
      "*Conceptual illustration of why vectorized operations are preferred over loops:*\n",
      "```python\n",
      "  # Slow loop-based approach (AVOID for large datasets):\n",
      "  # cases_per_million_list = []\n",
      "  # for index, row in df_full.iterrows():\n",
      "  #     if row['Population'] > 0:\n",
      "  #         cases_per_million_list.append((row['Confirmed'] / row['Population']) * 1_000_000)\n",
      "  #     else:\n",
      "  #         cases_per_million_list.append(0)\n",
      "  # df_full['Cases_Per_Million_Loop'] = cases_per_million_list\n",
      "```\n",
      "\n",
      "The direct column operation `(df_full['Confirmed'] / df_full['Population']) * 1_000_000` is highly optimized by Pandas/NumPy and executes much faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:109: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='float', errors='ignore')\n",
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:109: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='float', errors='ignore')\n",
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:109: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='float', errors='ignore')\n",
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:109: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='float', errors='ignore')\n",
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:104: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='integer', errors='ignore')\n",
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:109: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='float', errors='ignore')\n",
      "C:\\Users\\Ayanti\\AppData\\Local\\Temp\\ipykernel_14556\\834221502.py:104: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_full[col] = pd.to_numeric(df_full[col], downcast='integer', errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Define the file path for the uploaded CSV\n",
    "file_path = 'covid_project_dataset.csv'\n",
    "\n",
    "print(\"# Section 4: Handling Large Datasets (Python Code and Output)\")\n",
    "print(\"This section demonstrates techniques for handling large datasets, including chunk-based loading, memory optimization, and vectorized operations.\")\n",
    "\n",
    "print(\"\\n## 1. Chunk-based Loading and Processing\")\n",
    "print(\"This method is useful when your dataset is too large to fit into memory. We process it in smaller chunks.\")\n",
    "print(f\"\\n*Note: The file '{file_path}' might not be 'large' enough to strictly require chunking, but this code demonstrates the principle.*\")\n",
    "\n",
    "chunk_size = 50000  # Define a chunk size\n",
    "\n",
    "total_rows_processed = 0\n",
    "total_confirmed_cases_chunked = 0\n",
    "\n",
    "try:\n",
    "    print(f\"\\nReading '{file_path}' in chunks of {chunk_size} rows...\")\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        print(f\"\\n--- Processing Chunk {i+1} ---\")\n",
    "        print(f\"Chunk shape: {chunk.shape}\")\n",
    "\n",
    "        # Rename columns for consistency, assuming OWID-like structure as before\n",
    "        chunk.rename(columns={\n",
    "            'location': 'Country/Region',\n",
    "            'date': 'Date',\n",
    "            'total_cases': 'Confirmed',\n",
    "            'total_deaths': 'Deaths',\n",
    "            'total_recoveries': 'Recovered',\n",
    "            'population': 'Population' # Include population if available for later use\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Basic cleaning for chunk: Convert date, fill NAs for key columns\n",
    "        if 'Date' in chunk.columns:\n",
    "            chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n",
    "            chunk.dropna(subset=['Date'], inplace=True)\n",
    "        for col in ['Confirmed', 'Deaths', 'Recovered']:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = pd.to_numeric(chunk[col], errors='coerce').fillna(0)\n",
    "\n",
    "        # Example processing: sum confirmed cases in this chunk\n",
    "        if 'Confirmed' in chunk.columns:\n",
    "            chunk_confirmed_sum = chunk['Confirmed'].sum()\n",
    "            total_confirmed_cases_chunked += chunk_confirmed_sum\n",
    "            print(f\"Confirmed cases in this chunk: {chunk_confirmed_sum:.0f}\")\n",
    "\n",
    "        total_rows_processed += len(chunk)\n",
    "        print(f\"First 5 rows of Chunk {i+1}:\")\n",
    "        print(chunk.head())\n",
    "\n",
    "    print(f\"\\nFinished processing all chunks. Total rows processed: {total_rows_processed}\")\n",
    "    print(f\"Total confirmed cases aggregated from chunks: {total_confirmed_cases_chunked:.0f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found. Please ensure the CSV file is in the correct directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during chunked loading: {e}\")\n",
    "\n",
    "# --- Load the full dataset for memory optimization demonstration ---\n",
    "print(\"\\n## 2. Optimize Memory Usage\")\n",
    "print(\"We'll load the full dataset and then apply data type optimizations to reduce memory footprint.\")\n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv(file_path)\n",
    "    # Rename columns consistently for memory optimization part\n",
    "    df_full.rename(columns={\n",
    "        'location': 'country', # Corrected from 'Country/Region' to 'country' based on prior inspection\n",
    "        'date': 'Date',\n",
    "        'total_cases': 'Confirmed',\n",
    "        'total_deaths': 'Deaths',\n",
    "        'total_recoveries': 'Recovered',\n",
    "        'population': 'Population'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Convert Date column for consistency\n",
    "    if 'Date' in df_full.columns:\n",
    "        df_full['Date'] = pd.to_datetime(df_full['Date'], errors='coerce')\n",
    "        df_full.dropna(subset=['Date'], inplace=True)\n",
    "\n",
    "    # Initial memory usage\n",
    "    initial_memory_mb = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"\\nInitial memory usage of DataFrame: {initial_memory_mb:.2f} MB\")\n",
    "\n",
    "    # Apply memory optimization\n",
    "    for col in df_full.columns:\n",
    "        col_type = df_full[col].dtype\n",
    "\n",
    "        # Convert object columns to 'category' if suitable\n",
    "        if col_type == 'object':\n",
    "            num_unique_values = df_full[col].nunique()\n",
    "            num_total_values = len(df_full[col])\n",
    "            # A common heuristic: if unique values are less than 50% of total rows and less than 500\n",
    "            if num_unique_values / num_total_values < 0.5 and num_unique_values < 500:\n",
    "                df_full[col] = df_full[col].astype('category')\n",
    "                print(f\"  Converted '{col}' (object) to 'category' (Unique: {num_unique_values}).\")\n",
    "\n",
    "        # Downcast numerical columns\n",
    "        elif pd.api.types.is_numeric_dtype(col_type):\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                # Attempt to downcast integers\n",
    "                df_full[col] = pd.to_numeric(df_full[col], downcast='integer', errors='ignore')\n",
    "                if df_full[col].dtype != col_type:\n",
    "                    print(f\"  Downcasted '{col}' (int) from {col_type} to {df_full[col].dtype}.\")\n",
    "            elif pd.api.types.is_float_dtype(col_type):\n",
    "                # Attempt to downcast floats\n",
    "                df_full[col] = pd.to_numeric(df_full[col], downcast='float', errors='ignore')\n",
    "                if df_full[col].dtype != col_type:\n",
    "                    print(f\"  Downcasted '{col}' (float) from {col_type} to {df_full[col].dtype}.\")\n",
    "\n",
    "    # Final memory usage\n",
    "    final_memory_mb = df_full.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"\\nFinal memory usage of DataFrame after optimization: {final_memory_mb:.2f} MB\")\n",
    "    print(f\"Memory reduction: {((initial_memory_mb - final_memory_mb) / initial_memory_mb) * 100:.2f}%\")\n",
    "    print(\"\\nDataFrame dtypes after optimization:\")\n",
    "    print(df_full.dtypes)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found for memory optimization. Skipping this section.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during memory optimization: {e}\")\n",
    "\n",
    "print(\"\\n## 3. Implement Vectorized Operations using NumPy for Better Performance\")\n",
    "print(\"Pandas and NumPy are designed for vectorized operations, which are significantly faster than explicit Python loops for large datasets.\")\n",
    "print(\"Many operations in previous sections (like `.diff()`, `.groupby().agg()`, arithmetic operations on columns) already leverage vectorization.\")\n",
    "\n",
    "try:\n",
    "    # Ensure 'Confirmed' and 'Population' columns exist and are numeric\n",
    "    if 'Confirmed' in df_full.columns and 'Population' in df_full.columns:\n",
    "        df_full['Confirmed'] = pd.to_numeric(df_full['Confirmed'], errors='coerce').fillna(0)\n",
    "        df_full['Population'] = pd.to_numeric(df_full['Population'], errors='coerce').fillna(1) # Fill 1 to avoid div by zero\n",
    "\n",
    "        print(\"\\nDemonstrating vectorized operation: Calculating 'Cases per Million' (approximate, using total population).\")\n",
    "        # Vectorized operation\n",
    "        df_full['Cases_Per_Million'] = (df_full['Confirmed'] / df_full['Population']) * 1_000_000\n",
    "        print(\"\\nFirst 5 rows with 'Cases_Per_Million' calculated:\")\n",
    "        # Corrected column name: use 'country' instead of 'Country/Region'\n",
    "        print(df_full[['country', 'Date', 'Confirmed', 'Population', 'Cases_Per_Million']].head())\n",
    "\n",
    "        # Compare with a hypothetical loop (not executed for performance, but for illustration)\n",
    "        print(\"\\n*Conceptual illustration of why vectorized operations are preferred over loops:*\")\n",
    "        print(\"```python\")\n",
    "        print(\"  # Slow loop-based approach (AVOID for large datasets):\")\n",
    "        print(\"  # cases_per_million_list = []\")\n",
    "        print(\"  # for index, row in df_full.iterrows():\")\n",
    "        print(\"  #     if row['Population'] > 0:\")\n",
    "        print(\"  #         cases_per_million_list.append((row['Confirmed'] / row['Population']) * 1_000_000)\")\n",
    "        print(\"  #     else:\")\n",
    "        print(\"  #         cases_per_million_list.append(0)\")\n",
    "        print(\"  # df_full['Cases_Per_Million_Loop'] = cases_per_million_list\")\n",
    "        print(\"```\")\n",
    "        print(\"\\nThe direct column operation `(df_full['Confirmed'] / df_full['Population']) * 1_000_000` is highly optimized by Pandas/NumPy and executes much faster.\")\n",
    "    else:\n",
    "        print(\"Skipping 'Cases per Million' calculation: 'Confirmed' or 'Population' column not found in DataFrame or are not numeric after type conversion.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during vectorized operations demonstration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341eadf-bce9-46dc-a196-61637ea428bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
